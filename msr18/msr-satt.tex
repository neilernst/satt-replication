\documentclass[sigconf,natbib=false]{acmart}
\usepackage[autostyle]{csquotes}
\usepackage{todonotes}
\let\citename\relax
\RequirePackage[abbreviate=true, style=ACM-Reference-Format,dateabbrev=true, isbn=true, doi=true, urldate=comp, url=true, maxbibnames=9, backref=false, backend=biber]{biblatex}
\addbibresource{/Users/nernst/Documents/refs/bib/abbr.bib}
\addbibresource{msr-satt.bib}

\usepackage{booktabs} % For formal tables
\setcopyright{rightsretained}
\renewcommand{\bibfont}{\Small}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El Paso, Texas USA}
\acmYear{2018}
\copyrightyear{2018}

\begin{document}
\title{Bayesian Hierarchical Modelling for Tailoring Code Metric Thresholds} %in software metrics studies:: a replication and tutorial}


% \author{Neil A. Ernst}
% \orcid{0000-}
% \affiliation{%
%   \institution{Computer Science\\University of Victoria}
%   \city{Victoria}
%   \state{BC}
% }
% \email{nernst@uvic.ca}

\begin{abstract}
Software data is highly contextual. While there are ‘global’ lessons (otherwise no study would be generalizable), individual software projects exhibit many ‘local’ properties, making inter-project comparisons dangerous. A key problem is to construct locally useful prediction models, while still leveraging global characteristics. Previous work has tackled this problem with a combination of clustering and transfer learning approaches to identify locally similar characteristics. We apply a simpler probabilistic programming approach, known as hierarchical modeling, to support cross-project comparisons while preserving local context. To demonstrate the approach, we conduct a conceptual replication of an existing study on setting contextual metrics thresholds, focusing on Coupling Between Objects. Our hierarchical model reduces model error compared to a strictly global approach by up to 50\%.\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
\ccsdesc[500]{Mathematics of computing~Bayesian computation}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
\ccsdesc[300]{Software and its engineering~Object oriented architectures}


\keywords{hierarchical models, software metrics thresholds, probabilistic programming, architecture quality}

\maketitle

\section{Introduction}
Making locally-relevant statistical inferences by leveraging larger, cross-project datasets remains difficult. This is because the local context differs from the global context in many dimensions (shown in \cite{zhang13context} and termed the \emph{ecological inference problem} by Posnett \cite{Posnett:2011}). Individual projects may be written by different developers, use different software processes, have different lifespans, and so on. Using global lessons can be useful, however, since these individual projects often lack sufficiently large datasets to make statistical inference reliable. As a result, many software researchers, particularly in the defect prediction and effort estimation communities, have moved to cross-project approaches, including clustering (identifying all projects with similar characteristics) and transfer learning (finding a function that maps a classifier from one project onto the characteristics of another project). One approach that has not been attempted is Bayesian hierarchical modeling (also known as multi-level modeling or random effects modeling)\footnote{`modeling' is used here in the statistical sense, not the software abstraction sense}. A hierarchical model is one where the model parameters (mean and variance in the Normal distribution, for example) themselves have parameters. The hierarchy groups data by higher-level groups, e.g. files by project.

While popular in social sciences (one author has said ``multilevel modeling deserves to be the default form of regression \cite[p.14]{mcilreath16}'', this form of modeling is underused in the software engineering literature. Pickard, Kitchenham and Jones \cite{PICKARD1998811} point out that combining results in software engineering could leverage new (for 1998) advances in Bayesian hierarchical models. It does not seem as though there has been much take up of this call; in \S \ref{related} we highlight the one or two studies that have. Part of the problem has been that the computational power required for producing the posterior probability distributions is large (excessive for 1998), and only recently has fitting these models been possible on laptop CPUs. The simplest models in this paper, for example, required 26.9 seconds, and the most complex XXX.

%wanted to replicate this analysis using a hierarchical model. A hierarchical model is a way of modeling data using >1 \emph{level}. The classical examples are radon\footnote{http://mc-stan.org/documentation/case-studies/radon.html} and school performance \cite{gelman14}. %In the radon case, you have samples of radon (a colorless, odorless gas emitted from the earth) that are collected per house. We would like to make inferences about radon distributions across a state, so we could either just map out all the individual samples, or, using some knowledge of county distributions, we could use those to infer radon levels for places with fewer samples. For example, a county with one (outlier) sample we can *pull* back using the data from surrounding counties.

We investigate hierarchical models with a conceptual replication of the study of Aniche et al. \cite{Aniche2016} (henceforth SATT). The SATT study postulated that the \emph{architectural role} a class plays influences the expected value of certain software metrics. In particular, studying the Spring web framework, they hypothesized that whether a class was a Controller, Repository, or View (Spring-specific architectural roles) should influence its Chidamerer and Kemerer (CK) metric values \cite{Chidamber_1991}. For example, since a controller is interacting with views and models, presumably it should be more coupled. Their paper shows that this does seem to be the case for several CK metrics. They considerately make the dataset and scripts available for download\footnote{https://mauricioaniche.github.io/scam2016/}. We apply a hierarchical model to show how this approach can improve the bias and variance of threshold setting.

This paper introduces an established idea---hierarchical models---into a new domain, software engineering research. We begin by introducing some statistical background. We use the Stan probabilistic programming language \cite{Carpenter2017} for model inference. We apply the model to the SATT dataset  \cite{Aniche2016}. We show how our model is constructed, validate it with K-crossfold validation and root mean squared error, and discuss how such approaches might be applied in the future. An available Jupyter notebook demonstrates how this works in practice\footnote{removed for DBR}.

Along the way, we present some emerging results that show:
\begin{enumerate}
	\item Hierarchical models are simple and easy to explain;
	\item Probabilistic programming languages like Stan are easy to use  for  Bayesian data analysis;
	\item Hierarchical models support analysis that takes into account the rich diversity of empirical data in software engineering;
	\item Hierarchical models outperform purely local or purely global approaches.
\end{enumerate}

% This lends itself to a Bayesian analysis, and since I am learning Pandas and Stan, I thought it would make a nice case study to replicate this. Andrew Gelman covered this type of scenario directly [here](http://andrewgelman.com/2016/03/01/hes-looking-for-a-textbook-that-explains-bayesian-methods-for-non-parametric-tests/), and states "Just model what you want to model directly, and forget about the whole null hypothesis significance testing thing". So that's what I'll do.[^2]
\section{Background}
\subsection{Bayesian Hierarchical Models} % (fold)
For a general introduction to a Bayesian approach to statistics, people can refer to \cite{gelman14} and \cite{mcilreath16}. In general, however, a Bayesian approach allows us to ask the question ``What is the probability my hypothesis is true, conditional on the data". This follows from the standard Bayes theorem, $\frac{P(H | D) = P(D | H) * P(H)}{P(D)}$, where H is our hypothesis (i.e., architectural role influences software metrics), and D is the data we gather. Bayesian inference calculates a \emph{posterior probability distribution} $P(H|D)$. It does so as a consequence of the assumptions: $P(D | H)$,the \emph{likelihood}; the parameters we wish to estimate, and $P(H)$ our \emph{prior probability} for those parameters. We must explicitly assign a prior probability in Bayesian statistics, which can range from \emph{uninformative}, i.e., we claim no knowledge about what it should be, to strongly informative (we have a good idea of what the distribution of the data should be, e.g., mean and variance). The strength of our prior can be thought of as the extent to which new data influences our posterior. Bayesian inference is a machine that consumes data (\emph{conditions}) to generate a posterior, given the assumptions. This machine is often hard to construct mathematically, particularly in the hierarchical models we introduce in this paper. As a result, probabilistic programming techniques are needed to compute the posterior. 

%Contrast this with a frequentist or traditional approach. There we propose a null hypothesis $H_0$, and, based on assumptions that are typically unexplored, assign to that a probability distribution based on the observed data. We then conduct a statistical test to see if the outcome we observed in our sample can be explained by the null hypothesis being true. Typically "explained by" implies that this sample lies within 95\% of the probability density. Note that this being *false* does not imply our alternate hypothesis is true; merely that our null hypothesis is not explaining this result (very well). -->
\subsection{Probabilistic Programs} % (fold)
Probabilistic programming is a programming paradigm that uses an underlying inference engine to fit complex, multi-level models. The most obvious difference from traditional programming is that variables are not deterministic but rather stochastic; their value comes from a probability distribution. In order to compute this probability distribution, probabilistic programming languages use Markov Chain Monte Carlo (MCMC) sampling. 
It is only recently that computational power has caught up to the promise of probabilistic programs. For example, to calculate the fit for the pooled model (discussed below) requires \todo{fix} on a 2017 laptop\footnote{A good survey of probabilistic programming is available at \url{https://moalquraishi.wordpress.com/2015/03/29/the-state-of-probabilistic-programming/}}

% A multi-level or hierarchical model is one in which the distribution of the underlying parameters is itself something to be estimated from the data (hyper-parameters or hyper-priors). 


% By contrast with the programming most readers are familiar with, probabilistic programs are declarative and statistical. This means we define upfront what we believe the predictor can be modeled as, typically using some statistical models; we then turn to the inference engine to simulate some reasonably large number of draws against that declared model, typically using a variant of Markov Chain Monte Carlo estimators. The MCMC approach uses random sampling, optimization to condition model parameters on the prior data.

One example of a probabilistic programming language (and the one used in this paper) is Stan \cite{Carpenter2017}, and its PyStan library (the PyMC3 package is another variant). The Stan probabilistic program to compute a linear regression $y \sim \mathcal{N}(\mu,\sigma)$ with $\mu$, our linear model, represented by $\beta_1 + \beta_2 x_i$, looks like:

\begin{verbatim}
model {
  y ~ normal(beta[1] + beta[2] * x, sigma);
}
data {
  int<lower=0> N; 
  vector[N] x;
  vector[N] y;
}
parameters {
  vector[2] beta;
  real<lower=0> sigma;
} 
\end{verbatim}

where we provide input for the data section (i.e., our vector of metric values), and Stan does the inference to find values for the parameters section that maximize the posterior, conditioned on the data. We then call on PyStan to compile and sample the model:

\begin{verbatim}
compiled_model = pystan.stanc(model_code=pooled_data \
	 + pooled_parameters + pooled_model)
model = pystan.StanModel(stanc_ret=compiled_model)
pooled_fit = model.sampling(data=pooled_data_dict, \
	iter=1000, chains=2)

pooled_sample = pooled_fit.extract(permuted=True)
\end{verbatim}

where the variable \textsf{pooled\_fit} represents the sampled model using 2 runs of 1000 MCMC iterations; this produces a sample of 2000 potential values for our parameters (the posterior distribution), from which we then extract the means, standard errors, etc.

\section{Methodology}
The method we use in this paper is to 1) collect the data; 2) replicate the previous results to show the data are accurate; 3) fit a parametric model to the data; 4) apply three different regression models to the dataset, and then 5) compare the root mean squared error of each approach. We conclude by 6) showing how the regression model can be used to select metric thresholds.

\subsection{Collect Data}---The data already exists in nicely packaged form from the SATT authors~\cite{aniche16}. For the purposes of this short paper, we focus on a narrower hypothesis than SATT, restricting the analysis to the CK metric Coupling Between Objects (CBO) and the Spring Controller role. The original paper looked at more roles and more metrics, but the approach we describe would be easy to extend in this way. We have 120 projects in the SATT  dataset, 116 of which have Controllers, and a wide range of values for N (number of files). There are 56,106 files overall. Some projects are very tiny. The total file count of the smallest is 28, and the number of Controller files in that project (for example) is 18. We calculate the Mann-Whitney measure and Cliff's Delta measure for Controller metrics for all projects, compared to all other roles (including no role), and find a large effect (0.657), just like the original did. 

\subsection{Fitting a Probability Distribution}
The original study used a non-parametric test to show the effect of architectural role, but for our purposes we will need some probability distribution to serve as the likelihood. Herraiz et al. \cite{Herraiz2012} have claimed a `double-pareto'---double Pareto-logNormal---model best explains the data they looked at (Java source code from the Qualitas corpus). This is a model with a lognormal head and a powerlaw tail. Since that makes the later models more complex to understand, we stick with the lognormal, as did Aniche et al. \cite{Aniche2016}.

% \begin{figure}[tb]
% 	\centering
% 	\includegraphics[width=\columnwidth]{fit-distr}
% 	\caption{Distribution fits}
% 	\label{fig:distr}
% \end{figure}
% Fig. \ref{fig:distr} shows the SATT dataset as a histogram, using a Scipy model fitting function. This tells us to use an inverse gamma, but the lognormal is nearly as good a fit. We therefore transform CBO into a log CBO (LCBO metric, in order to work with it as a normal distribution.
 % We can also use the probplot approach with Quantile-Quantile graphs. Here we compare our data to the theoretical model of each distribution. A perfect fit implies a diagonal 45 degree match. QQ plots show divergence from the predicted (theoretical) values. 
\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{violin.pdf}
	\caption{Distribution and mean for LCBO. 1=Controller. 0=no role.}
	\label{fig:violin}
\end{figure}
%To wit:

Fig \ref{fig:violin} shows violin plots for the LCBO values of each separate role (1=Controller). Note that in the figure the long negative tail reflects files with a CBO of 0 (we transform by taking log(0.1)).

\section{Model Fitting}
Accounting for our focus on Controller and CBO, the rephrased research question of the SATT paper is
\begin{quote}
Is there a difference in CBO between Spring Controller classes and all other classes?
\end{quote}
The SATT finding was that a file's architectural role played a significant part in its eventual CBO numbers. They used the procedure of Alves et al. \cite{alves2010deriving} to identify thresholds. The Alves thresholds were set by ranking files (across all projects) by LOC, then finding the metric value of the file closest to 70/80/90\% (moderate/high/very high risk). The percentage is of the total lines of code. Aniche et al. extend this, stratifying the files by architectural role, so we end up with 6 sets of 3 thresholds, one set per architectural role. To identify if a file violates a threshold, one looks up its role, and then retrieves the three thresholds and compares the file's metric against the thresholds.

In our approach, we will produce a regression model that produces a posterior probability distribution of CBO scores. This will be global and per-project. We will set our thresholds based on 70/80/90\% of the probability mass. To find if a given file violates the threshold, one looks up that project's predicted CBO for Controller or non-Controller files, finds the threshold predicted, and checks the file's actual CBO. In the models below we do not use LOC as a regression predictor, but that is a simple extension.

The hierarchy in our hierarchical model is files within projects.  

\subsection{Global Pooling}
Full pooling is a global assessment of differences. All individual files are aggregated and we fit a single regression model (a fixed effects model). We predict CBO score, $y$, as a function of architectural role: 
$ y_i = \alpha + \beta x_i + \epsilon_i$.

First, we model the data in Stan, following the model approach described by Chris Fonnesbeck\footnote{https://github.com/fonnesbeck/stan\_workshop\_2016/blob/master/notebooks/Multilevel\%20Modeling.ipynb}. In the Bayesian context, what we have created is a posterior probability distribution for the unknown parameters $\beta$ and $\sigma$. We aren't getting point estimates, but rather distributions. Hence, in order to get a sense for what the probability masses around (due to the central limit theorem, we get normal distributions for these parameters), we sample from the posterior distribution and show the range of values.

% Fig. \ref{fig:pool-regress} shows a regression line, using the new parameters for the slope and intercept of the regression line. 
The regression result says that in a pooled model, we would expect a mean LCBO of 1.144 (CBO of 3.14) for non-controllers; and the controller predictor has a mean CBO of \textsf{exp(1.144+0.0291) = 4.20}. Note that replicating this with ordinary least squares (OLS) regression produces the same parameters. OLS cannot manage multiple regression levels.

% \begin{figure}[tb]
% 	\centering
% 	\includegraphics[width=\columnwidth]{pooled-regress}
% 	\caption{Pooled regression model. X-Axis reflects Non-Controller/Controller. Y-Axis is LCBO}
% 	\label{fig:pool-regress}
% \end{figure}

\subsection{Unpooled}
We could go to the other extreme, and model the data per project (120 projects). Each project gets a separate regression model, and we can compare the co-efficients for each one. The regression looks like $y_i = \alpha_{i[j]} + \beta x_i + \epsilon_i $

The difference between these two approaches is a bias-variance tradeoff. In the fully pooled model, we bias the data to the larger instances (the value for the number of files per project, which is our number of unique measures, range from 26 files to 2677 files). The subtle variance per project, which may demonstrate confounding effects (e.g., that some unmodelled predictor is partly responsible for the value of LCBO), is ignored in this model.

\todo[inline]{clunky}
By contrast, in the unpooled approach, we allow total variance, at the expense of high variance (and possibly misleading data) in the smaller samples. McIlreath (\cite{mcilreath16}) calls this ``anterograde amnesia" since our model machinery forgets what it learned from the previous projects. 

\subsection{Partial Pooling - Varying Slope and Intercept}
% We now have two different models of what we would expect a file's LCBO to be. In the \emph{pooled} approach, it is a function of the global distribution of LCBO metric scores for controller and non-controller, averaged over all projects. In the \emph{unpooled} approach, a file's CBO score would be a function of the CBO distribution over all other files \emph{in that project}. 

Partial pooling, as you might expect, combines the two previous approaches. It gives nuance we think exists because of local effects from architectural role, but regularizes that by the global parameters (another name for this is \emph{shrinkage}). We are adjusting our local estimate \emph{conditional} on the global estimate (which itself is conditioned on the new local values it encounters). The simplest approach to partial pooling is to take a weighted average and apply that to the global estimate. Smaller samples then have less impact on our global estimate. This is similar `regression to the mean' idea of \cite{JORGENSEN2003} but calculated more systematically.

The partial pooling model will allow the regression line's slope and intercept to vary based on data from the specific project. If our pooled regression equation predicts $y_i$, an observation at point $i$, to be  $y_i = \alpha + \beta x_i + \epsilon_i$, than the partial pooling regression equation is $y_i = \alpha_{j[i]} + \beta_{j[i]} x_{i} + \epsilon_i$. 

The model component of the probabilistic program is 
\begin{verbatim}
model {
  mu_a ~ normal(0, 100);
  mu_b ~ normal(0, 100);
  a ~ normal(mu_a, sigma_a);
  b ~ normal(mu_b, sigma_b);
  y ~ normal(a[project] + b[project].*x, sigma);
}
\end{verbatim}

with parameters $a,b$ having posteriors that are normal, with priors for the mean distributed as another normal model with mean 0 and sigma 100. This is an uninformative prior (allowing for a wide range of values, and thus dominated by the data). Choosing priors effectively is a core skill in probabilistic programming. 

\textbf{Analyzing the model} An important part of Bayesian hierarchical modeling is model checking. This involves some inspection of the model inference process to ensure the fit is reasonable. To do this we plot the posterior distribution of our parameters and the individual samples we drew, shown in Fig. \ref{fig:modelcheck}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{eval-fit}
	\caption{Checking model convergence.}
	\label{fig:modelcheck}
\end{figure}

The left side shows our marginal posterior---for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is. The right side shows the sampling chains. Our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns) \cite{Sorensen16}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{slopes}
	\caption{Visualizing the varying slope/intercept model. Each line is a project-level regression line.}
	\label{fig:slopes}
\end{figure}

We can visualize the effect of a varying slope, varying intercept model in Fig. \ref{fig:slopes}. A sample of five projects in the SATT dataset is shown in Fig. \ref{fig:partial-pool}. The dashed black line is the partial pooling estimate; the dashed red line is the global pooled estimate; and the solid blue line the unpooled estimate. The global estimate seems quite wrong, while depending on the number of data values, the partial and unpooled estimates are fairly close. The utility of the partial pooling approach increases as the number of datapoints decreases.

\begin{figure*}[tb]
	\centering
	\includegraphics[width=\textwidth]{partial-pool.pdf}
	\caption{Comparing three models. Blue=unpooled, dotted black=partial pool, dashed red=pooled}
	\label{fig:partial-pool}
\end{figure*}

\section{Validation}
%Validation is a bit trickier in hierarchical models. In K-Fold cross validation the temptation is to do this at the file level (i.e., hold back 10\% of the files for validation). The trouble is this may be unrepresentative of the higher levels of structure (the projects). Accordingly we hold back 1/5

We validate using a simple mean squared error (RMSE) approach.% with 5-fold cross-validation. 
We pull out a random sample of individual points, stratified by project, fit our three regression models, then calculate the average distance from the predicted score to the actual, held-out points. We expect the partial-pooling to have a lower RMSE value.

Table \ref{tbl:rmse} reports the mean RMSE over all projects with Controllers (n=115), and approximate Stan model training time with 2 chains of 1000 iterations. Partial pooling is nearly twice as effective as full pooling, but takes 4 times as long (however, an OLS approach is nearly instantaneous and produces the same RMSE as the full pooling model). No pooling is nearly as effective (using only local data). However, 

\begin{table}
\begin{tabular}{c|c|c}
Model & RMSE & Approx Training Time (s) \\ \\
Full Pool & 1.099 & 26.9 \\
Unpooled & 0.630 & 119.8 \\
Partial Pool & 0.523 & 217.4 \\

\end{tabular}
\label{tbl:rmse}
\caption{RMSE for 3 Regression Approaches}
\end{table}

\section{Justifying Thresholds}
In the Aniche et al. paper \cite{Aniche2016}, the process for deriving thresholds was as follows. First, thresholds were assigned per architectural role. That is, one would have a different CBO threshold for classes that were annotated \textsf{@Controller} than \textsf{@Service}. Secondly, the authors did a non-parametric Mann-Whitney comparison to see whether there was a significant difference in the CBO scores for each role, as compared to the other classes in the benchmark. If the effect size was medium or higher, they proceeded. From there the process was to rank the benchmark classes in the role, normalize by LOC, compute a ranked sum of all classes, and extract the class with metric value that made up 70/80/90\% of the benchmark role, respectively. 

We use our partial pooling regression model to set thresholds at 70/80/90\% of the normal distribution of log CBO. E.g., for the project `V2V', the partial pooling model tells us the project's mean LCBO value $y \sim \mathcal{N}(a_\text{project} + b_\text{project}*x, \sigma)$. We can create a simple normal distribution and find the point probability for our thresholds $\tau$ (i.e. the point on the normal distribution holding $\tau$\% of the probability mass). For `V2V' this translates to expected Controller CBO scores (not log) of 32,49, and 88. Compare this to another project, `tatami-team', with 21,32,58, respectively.  It is an open question to what extent there is a global CBO standard that ought to apply. %2gx_demo = 20,31,56

\section{Related Work}
\label{related}
There are two main categories of related work. The background section earlier already introduced the concepts of Bayesian hierarchical modeling and probabilistic programs, so we avoid repeating that here. 

First, there is work that identifies the \textbf{differences between per-project and pooled predictors} in analysing software metrics. Menzies et al. \cite{menzies11ase} introduced the concept of `local' (clusters of data with similar characteristics) and `global' (cross-project, pooled) lessons in defect prediction. This idea has been followed up by many others (e.g., \cite{Kamei2016,Bettenburg2012,panichella14}). Posnett et al. \cite{Posnett:2011} conducted a study comparing defect prediction at two levels: aggregated (package) vs. unaggregated (file) and introduced the concept of ecological inference. The innovation in our paper is to propose a third level which blends the two others (partial pooling). Our paper does not, however, make any claim to improve on defect prediction---we leave that to future work. 

The Menzies et al. paper makes the case for considering both levels in analyzing metrics for defect prediction, and has been followed up with research into using \emph{transfer learning} for defect prediction \cite{ma2012transfer}. Transfer learning is similar to multi-level/hierarchical regression in fitting a model to a particular set of data. It then differs by learning key tuning parameters for the model when it is applied to new datasets. The intent is to avoid overfitting on sparse datasets by learning a generic model from more populous data. The two approaches share a similar goal; the main advantage to multi-level regression is that the models are conceptually simpler, and well supported by tooling. This is, however, a personal opinion. Many papers (such as \cite{Zhou15regress}) have leveraged linear (continuous response) and logistic regression (categorical response); however, these are single-level models, in that they operate on predictors a pool of data (all files in a project, all projects in an ecosystem, etc). Complex classifiers are also less explainable compared to hierarchical models, which clearly delineates the likelihood, priors and parameters.

A similar approach to ours---in that it uses linear regression---is found in \cite{Bettenburg2012}. They show that one challenge is identifying levels in software data, i.e., discovering sets of projects that can be said to be similar in some way. This \emph{clustering} problem is one our current paper does not tackle, relying as it does on pre-existing clusters (i.e., controller annotated Java files in Spring projects). Like previous work, they attack the problem with linear regression at two levels, `global' and `local'. However, rather than our multi-level models, they attempt to blend the levels using a different regression approach, Multivariate Adaptive Regression Splines (MARS). The MARS approach appears to be fairly uncommon in the statistics literature, and uses hinge functions to reduce the residual error. The Stan models we introduce are simpler to specify, and the Bayesian approach to conditioning on the prior data we find more intuitive.

Jørgenson et al. \cite{JORGENSEN2003} introduced a technique called `regression to the mean' (RTM). It also attempts to regularize local predictions using global parameters. It does this by finding a global mean and adjusting local estimates to that mean. Hierarchical models are a more generalized and robust version of this, at the expense of computation time.

Work from Ehrlich and Cataldo \cite{Ehrlich:2012} in 2012 used multi-level (hierarchical) models to assess communication in global software development. Levels were individuals, teams, and iterations. Their models are not specified in the text, but they fit a negative binomial regression model in what seems to be a frequentist approach (i.e., using maximum likelihood). Our Bayesian approach better accounts for prior information that is helpful when lacking observations.

Secondly, there is work that looks at \textbf{metric thresholds}. Since metrics were proposed for software, researchers have sought to identify what thresholds should trigger warnings. The paper we replicate from Aniche et al. \cite{Aniche2016} has a good survey. We point out the work of Herraiz et al. \cite{Herraiz2012}, Foucault et al. \cite{Foucault2014} and Alves et al. \cite{alves2010deriving} as more recent examples. The metrics industry, e.g. SIG and CAST, has also long used databases of company performance as benchmarks for evaluating new clients.

%TODO I have no good theory for the difference between a statistical classifier and linear regression

\section{Discussion}
The chief threats to validity in Bayesian hierarchical modeling come from poor understanding of the underlying probabilistic program, including what priors to choose. Model checking is vital to mitigate this. There are subtleties in how the sampler works that need careful checking, as shown by Wiecki's blog post \cite{Wiecki17}. Tuning parameters, such as the number of chains to run, needs practice. 

Other threats to validity include data analysis and implementation issues (internal validity). One threat that is mitigated by this approach is external validity, since a hierarchical model inherently accounts for inter-project differences. Indeed, additional levels are possible beyond files/projects (Spring, Java, etc).

This paper has scratched the surface of the utility of hierarchical modeling. Other useful aspects include the ability to handle correlations between local and global variables \cite{mcilreath16}, innately accounting for multiple-comparisons problems \cite{Gelman:2012aa}, and adding group-level predictors (for example, using number of files per project). Other aspects to explore include adding additional predictors such as lines of code, different underlying prior distributions, and improved analysis of model fit using Leave One Out (LOO) or Widely Applicable Information Criterion (WAIC) \cite{vehtari15}. Although consumer-ready, consumer-level Bayesian inference remains unexplored compared to the highly usable frequentist packages. Probabilistic programming is also an area that merits more investigation as a form of software development in its own right.

\section{Conclusion}
In this paper we introduced a hierarchical approach to setting metric thresholds. Using a partial pooling approach, we are able to account for global context, while retaining local detail. Our linear regression model defined a project-specific mean LCBO score, which we used to set the benchmarks accordingly. The hierarchical model is simple to describe, handles cross-project comparisons, and is more accurate than a purely global or purely local approach.

\section{Acknowledgements}
We are grateful to the authors of \cite{Aniche2016}, who made their well-packaged and documented data and code available for others.
 
\newpage
 \printbibliography 


\end{document}
