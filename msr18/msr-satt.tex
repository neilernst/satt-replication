\documentclass[sigconf,natbib=false]{acmart}
\usepackage[autostyle]{csquotes}
\usepackage{todonotes}
\let\citename\relax \RequirePackage[abbreviate=true,
style=ieee,dateabbrev=true, isbn=true, doi=false, urldate=comp,
url=true, maxbibnames=9, backref=false, backend=biber]{biblatex}
\addbibresource{/Users/nernst/Documents/refs/bib/abbr.bib}
\addbibresource{msr-satt.bib}

\usepackage{booktabs} % For formal tables
\copyrightyear{2018} 
\acmYear{2018} 
\setcopyright{acmlicensed}
\acmConference[MSR '18]{MSR '18: 15th International Conference on Mining Software Repositories }{May 28--29, 2018}{Gothenburg, Sweden}
\acmBooktitle{MSR '18: MSR '18: 15th International Conference on Mining Software Repositories , May 28--29, 2018, Gothenburg, Sweden}
\acmPrice{15.00}
\acmDOI{10.1145/3196398.3196443}
\acmISBN{978-1-4503-5716-6/18/05}


\begin{document}
\title{Bayesian Hierarchical Modelling for Tailoring  Metric Thresholds} %in software metrics studies:: a replication and tutorial}


\author{Neil A. Ernst}
\orcid{0000-0001-5992-2366}
\affiliation{%
  \institution{Department of Computer Science -- University of Victoria}
  % \city{Victoria}
  % \state{BC}
}
\email{nernst@uvic.ca}

\begin{abstract}
Software is highly contextual. While there are cross-cutting `global' lessons,
individual software projects exhibit many `local' properties. This data
heterogeneity makes drawing local conclusions from global data dangerous.
%Nonetheless, statistical inference in software is greatly improved with larger
%volumes of data. 
A key research challenge is to construct locally accurate
prediction models that are informed by global characteristics and data volumes.
Previous work has tackled this problem using clustering and transfer learning
approaches, which identify locally similar characteristics. This paper applies a
simpler approach known as Bayesian hierarchical modeling. We show that
hierarchical modeling supports cross-project comparisons, while preserving local
context. To demonstrate the approach, we conduct a conceptual replication of an
existing study on setting software metrics thresholds. Our emerging results show
our hierarchical model reduces model prediction error compared to a global
approach by up to 50\%.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
\ccsdesc[500]{Mathematics of computing~Bayesian computation}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
% \ccsdesc[300]{Software and its engineering~Object oriented architectures}
\keywords{hierarchical models, metrics thresholds, probabilistic programming}%, architecture quality}

\maketitle

\section{Introduction}

There are two problems with doing statistical inference on software projects.
The first is that finding small \emph{true} effects is often only possible with
large datasets, yet many software projects are comparatively small. The second
problem is that software characteristics (such as object oriented metrics)
differ from one project to the next \cite{zhang13context}. Posnett
\cite{Posnett:2011} terms this the \emph{ecological inference problem}. As a
result, many software researchers, particularly in the defect prediction and
effort estimation communities, have moved to cross-project approaches, including
clustering (identifying all projects with similar characteristics) and transfer
learning (finding a function that maps a classifier from one project onto the
characteristics of another project). Both have underlying models that are hard
to interpret, and ignore domain-specific knowledge about how software is
constructed. 
% %The clustering approaches are high-dimensional, and pay little
% attention to software process and data characteristics from past studies. %The
% unsupervised learning approach has been extremely successful, and applying deep
% neural nets to software data is proving powerful. 

This paper's new idea is to use a simpler and more understandable model for
learning from software data, Bayesian hierarchical models (also known as
multi-level models or random effects models \cite{Sorensen16})\footnote{`modeling' is used here in
the statistical sense, not the software abstraction sense}. A hierarchical model
is one where the model parameters themselves have parameters drawn from a
probability distribution. Parameters are things like mean and variance in a
Normal distribution. Hierarchical models calculate project-specific metrics,
while still taking into account the global ecological commonalities. Among other
benefits, this means that inference on projects with small datasets (level 1)
can be improved by our knowledge of the global set of data (level 2). 
%Using a hierarchical model, we are able to account for global context, while
%retaining local detail.

Our prediction task is to find the mean Coupling Between Objects metric for an
individual project. To solve this prediction task, we compare hierarchical
modeling to two other linear regression approaches. A \emph{global} model
(approach 1) pools all file information into a single level. Any interesting
project-level variance is lost. A \emph{local} or unpooled model (approach 2)
calculates local-only regression coefficients. No global-level data is used, so
it exhibits `anterograde amnesia'---any lessons from analyzing previous projects
are forgotten. Projects with few data points will be untrustworthy (high
standard errors). Finally, a \emph{hierarchical}, or partial pooling model
(approach 3) fits a linear model per project, but regularizing the prediction
using the global data. 

We find that partial pooling is much better than the other two approaches, using
root mean squared error (RMSE). The advantage of the partial pooling approach: 
\begin{enumerate}
	\item We have an explicit prior, so it is easy to understand what is in the model and how the model is created. 
	\item Sophisticated tooling supports hierarchical models (e.g., Stan \cite{Carpenter2017}), so off
	the shelf solutions are available.
	\item Partial pooling accommodates our intuitive understanding of software development analysis, that balances the local and global data available.
\end{enumerate}

While popular in social sciences (one author has said ``multilevel modeling
deserves to be the default form of regression \cite[p.14]{mcilreath16}'', this
form of modeling is underused in the software engineering literature. In 1998
Pickard, Kitchenham and Jones \cite{PICKARD1998811} pointed out that combining
results in software engineering could leverage new advances in Bayesian
hierarchical models. It does not seem as though there has been much take up of
this call; in \S \ref{related} we highlight the one or two studies that have.
The main issue has been the computational power required for producing the
posterior probability distributions. As we show, this is now resolved with
modern inference engines and processor power. 
%Like any Bayesian inference, it also demands we define our assumptions in the
%form of priors. % is large (excessive for 1998), and only recently has fitting
%these models been possible on laptop CPUs. The simplest models in this paper,
%for example, required 26.9 seconds, and the most complex just under 7 minutes.

We investigate hierarchical models with a conceptual replication of the study of
Aniche et al. \cite{Aniche2016} (henceforth SATT). The SATT study postulated
that the \emph{architectural role} a class plays (in particular, Spring
framework Controllers, Views, Repositories) influences the expected value of
certain software metrics. They assign a different threshold for various
Chidamerer and Kemerer (CK) metric values \cite{Chidamber_1991} based on the
role a file plays. We show how thresholds can be set more accurately using
hierarchical models.

 % In particular, studying the Spring web framework, they hypothesized that
 % whether a class was a Controller, Repository, or View (Spring-specific
 % architectural roles) should influence its . For example, since a controller
 % is interacting with views and models, presumably it should be more coupled.
 % Their paper shows that this does seem to be the case for several CK metrics.
 % They considerately make the dataset and scripts available for
 % download\footnote{https://mauricioaniche.github.io/scam2016/}. We apply a
 % hierarchical model to show how this approach can improve the bias and
 % variance of threshold setting.

%This paper introduces an established idea---hierarchical models---into a new domain, software engineering research. 
We begin by introducing some statistical background. 
%We use the Stan probabilistic programming language  for model inference. 
We apply a hierarchical model to the SATT dataset  \cite{Aniche2016}. We show
how our model is constructed, validate it with RMSE, and discuss how such
approaches might be applied in the future. An available Jupyter notebook
demonstrates how this works in
practice\footnote{\url{https://figshare.com/s/fd34d562ce882d1ab4d2}}.
Our emerging results show:
\begin{enumerate}
	\item Hierarchical models are easy to explain and set up;
	\item An example of using a probabilistic programming language for Bayesian data analysis;
	\item Hierarchical models support analysis that takes into account the rich diversity of empirical data in software engineering;
	\item Hierarchical models outperform purely local or purely global approaches.
\end{enumerate}

% This lends itself to a Bayesian analysis, and since I am learning Pandas and
% Stan, I thought it would make a nice case study to replicate this. Andrew
% Gelman covered this type of scenario directly
% [here](http://andrewgelman.com/2016/03/01/hes-looking-for-a-textbook-that-explains-bayesian-methods-for-non-parametric-tests/),
% and states "Just model what you want to model directly, and forget about the
% whole null hypothesis significance testing thing". So that's what I'll do.[^2]
\section{Background}
For a general introduction to a Bayesian approach to statistics, people can
refer to \cite{gelman14} and \cite{mcilreath16}. Bayesian inference is built on
Bayes's theorem, $P(H | D) = \frac{ P(D | H) * P(H)}{P(D)}$, where H is our
hypothesis (i.e., architectural role influences software metrics), and D is the
data we gather. Bayesian inference calculates a \emph{posterior probability
distribution} $P(H|D)$. It does so as a consequence of the assumptions: $P(D |
H)$, the \emph{likelihood}; the parameters we wish to estimate; and $P(H)$ our
\emph{prior probability} for those parameters. We must explicitly assign a prior
probability in Bayesian statistics. 
%, which can range from \emph{uninformative}, i.e., we claim no knowledge about
%what it should be, to strongly informative (we have a good idea of what the
%distribution of the data should be, e.g., mean and variance). The strength of
%our prior can be thought of as the extent to which new data influences our
%posterior. 
Bayesian inference is a machine that \emph{conditions} on the data to generate a
posterior, given the assumptions. This machine is often hard to construct
mathematically, particularly in the hierarchical models we introduce in this
paper. As a result, probabilistic programming techniques are needed to compute
the posterior. 
%Current languages use Hamiltonian Monte Carlo to do this.

%Contrast this with a frequentist or traditional approach. There we propose a
%null hypothesis $H_0$, and, based on assumptions that are typically unexplored,
%assign to that a probability distribution based on the observed data. We then
%conduct a statistical test to see if the outcome we observed in our sample can
%be explained by the null hypothesis being true. Typically "explained by"
%implies that this sample lies within 95\% of the probability density. Note that
%this being *false* does not imply our alternate hypothesis is true; merely that
%our null hypothesis is not explaining this result (very well). -->

Probabilistic programming is a programming paradigm that uses an inference
engine to fit complex and multi-level models. Variables are not deterministic
but rather stochastic. Their value comes from a probability distribution. In
order to compute this probability distribution, probabilistic programming
languages use Markov Chain Monte Carlo (MCMC) sampling (specifically,
Hamiltonian Monte Carlo and the No U-Turn Sampler). 
%It is only recently that computational power has caught up to the promise of
%probabilistic programs. For example, to calculate the fit for the pooled model
%(discussed below) requires 30 seconds to compile and 27 seconds to fit, on a
%2017 laptop\footnote{
A good survey of probabilistic programming is available at \cite{Quarashi}.
One example of a probabilistic programming language (and the one used in this
paper) is Stan \cite{Carpenter2017}, and its PyStan library (the PyMC3 package
is another variant). The Stan probabilistic program to compute a linear
regression $y \sim \mathcal{N}(\mu,\sigma)$ with $\mu$, our linear model,
represented by $\beta_1 + \beta_2 x_i$, looks like:

\begin{verbatim}
model {  y ~ normal(beta[1] + beta[2] * x, sigma); }
data {
  int<lower=0> N; 
  vector[N] x;
  vector[N] y; }
parameters {
  vector[2] beta;
  real<lower=0> sigma; } 
\end{verbatim}

where we provide input for the data section (e.g., our vector of metric values),
and Stan does the inference to find values for the parameters section that
maximize the posterior, conditioned on the data. 
%We then call on PyStan to compile and sample the model:

% \begin{verbatim}
% compiled_model = pystan.stanc(model_code=pooled_data \
% 	 + pooled_parameters + pooled_model)
% model = pystan.StanModel(stanc_ret=compiled_model)
% pooled_fit = model.sampling(data=pooled_data_dict, \
% 	iter=1000, chains=2)

% pooled_sample = pooled_fit.extract(permuted=True)
% \end{verbatim}

% where the variable \textsf{pooled\_fit} represents the sampled model using 2
% runs of 1000 MCMC iterations; this produces a sample of 2000 potential values
% for our parameters (the posterior distribution), from which we then extract
% the shape and location parameters like mean and standard error.

\section{Methodology}
Our research question is whether a hierarchical model is more accurate than a
pooled or unpooled regression model. As a working example, we perform a
conceptual replication of the study of Aniche et al. \cite{Aniche2016} (SATT).
This study explored how metrics thresholds could be contextualized based on
architectural role. We use their data to fit three different regression models,
then validate the accuracy of these models using RMSE. Once we have a model, we
can use that to estimate threshold values (i.e., level at which to take action
on refactoring). To identify if a file violates a threshold, one looks up its
role, and then retrieves the three thresholds and compares the file's metric
against the thresholds.

We focus on a narrower analysis than SATT, restricting it to the Chidamber and
Kemerer (CK) metric Coupling Between Objects (CBO) \cite{Chidamber_1991} and the
Spring Controller role. We choose CBO since the SATT results found it highly
correlated to architectural role, but our basic approach is easily extended. We
have 120 projects in the SATT dataset, 115 of which have Controllers, and a wide
range of values for number of files. There are 56,106 files overall. Some
projects are very tiny. The total file count of the smallest is 28, and the
number of Controller files in that project (for example) is 18 (recall that
handling such small datasets is one of the strengths of hierarchical models). We
calculate the Mann-Whitney measure and Cliff's Delta measure for Controller
metrics for all projects, compared to all other roles (including no role), and
find a large effect (0.657), just like the original paper did. 

The original study used a non-parametric test to show the effect of
architectural role, but for our purposes we will need a probability distribution
to serve as the likelihood. 
%Herraiz et al. showed a double
% Pareto-logNormal model best explains the data they looked at (Java source code
% from the Qualitas corpus). This is a model with a lognormal head and a powerlaw
% tail. Since that makes the later models more complex to understand, 
We choose a lognormal, as this is empirically accurate \cite{Herraiz2012}, and the same as that
chosen in  the SATT study \cite{Aniche2016}. We begin by taking the log of a
file's CBO score (LCBO).

% \begin{figure}[tb] \centering \includegraphics[width=\columnwidth]{fit-distr}
%   \caption{Distribution fits} \label{fig:distr} \end{figure} Fig.
%   \ref{fig:distr} shows the SATT dataset as a histogram, using a Scipy model
%   fitting function. This tells us to use an inverse gamma, but the lognormal
%   is nearly as good a fit. We therefore transform CBO into a log CBO (LCBO
%   metric, in order to work with it as a normal distribution. % We can also use
%   the probplot approach with Quantile-Quantile graphs. Here we compare our
%   data to the theoretical model of each distribution. A perfect fit implies a
%   diagonal 45 degree match. QQ plots show divergence from the predicted
%   (theoretical) values. \begin{figure}[tb] \centering
%   \includegraphics[width=.8\columnwidth]{violin.pdf} \caption{Distribution and
%   mean for LCBO} \label{fig:violin} \end{figure} %To wit:

% Fig \ref{fig:violin} shows violin plots for the LCBO values of each separate
% role. The long negative tail reflects files with a CBO of 0 (log(0.1)).

\subsection{Modeling the Problem}
We will use a simple linear regression model to estimate the posterior
distribution of (L)CBO values, and do this in three different ways. We follow
the model approach of Chris Fonnesbeck \cite{fonnesbeck2016}. Fig.
\ref{fig:partial-pool} on page 3 shows the overall comparison of the three
models.

\noindent\textbf{Global Pooling}
Global pooling aggregates all individual files and fits a single linear
regression model (a fixed effects model). We predict LCBO score, $y$, as a
function of whether the file is a controller (0/1), $x$: $ y_i = \alpha + \beta
x_i + \epsilon_i$, with equivalent statistical model expressed as $y \sim \mathcal{N}(\alpha
+ \beta x, \sigma)$.

What we have created is a posterior probability distribution for the unknown
parameters $\alpha, \beta, \sigma$. We are not getting point estimates, but
rather distributions. To obtain the parameters and errors, we sample from the
posterior distribution, i.e., find the mean and S.E. of the samples.

% Fig. \ref{fig:pool-regress} shows a regression line, using the new parameters
% for the slope and intercept of the regression line. The regression result says
% that in a pooled model, we would expect a mean LCBO of 1.144 (CBO of 3.14) for
% non-controllers; and the controller predictor has a mean CBO of
% \textsf{exp(1.144+0.0291) = 4.20}. Note that replicating this with ordinary
% least squares (OLS) regression produces the same parameters. OLS, however,
% cannot manage multiple regression levels.

% \begin{figure}[tb]
% 	\centering
% 	\includegraphics[width=\columnwidth]{pooled-regress}
% 	\caption{Pooled regression model. X-Axis reflects Non-Controller/Controller. Y-Axis is LCBO}
% 	\label{fig:pool-regress}
% \end{figure}

\noindent\textbf{Unpooled}
The other extreme from global pooling is to model the data per project (N=115).
Each project gets a separate regression model, and we can compare the
co-efficients for each one. The regression model is $y_i = \alpha_{i[j]} + \beta
x_i + \epsilon_i $ for projects $j$.

% The difference between these two approaches is a bias-variance tradeoff. In
% the fully pooled model, we bias the data to the larger instances (the value
% for the number of files per project, which is our number of unique measures,
% range from 26 files to 2677 files). The subtle variance per project, which may
% demonstrate confounding effects (e.g., that some unmodelled predictor is
% partly responsible for the value of LCBO), is ignored in this model.

Here we are favoring individual project variance. Small samples will greatly
bias our project-level estimates. McIlreath \cite{mcilreath16} calls this
``anterograde amnesia" since our model machinery `forgets' what it learned from
the previous projects. 

\noindent\textbf{Partial Pooling - Varying Slope and Intercept}
% We now have two different models of what we would expect a file's LCBO to be.
% In the \emph{pooled} approach, it is a function of the global distribution of
% LCBO metric scores for controller and non-controller, averaged over all
% projects. In the \emph{unpooled} approach, a file's CBO score would be a
% function of the CBO distribution over all other files \emph{in that project}. 
The hierarchical, \emph{partial pooling} approach adjusts the local estimate
\emph{conditional} on the global estimate. The hierarchy in our hierarchical
model is files within projects.  Partial pooling gives nuance we know exists
because of local effects from architectural role \cite{Aniche2016}, but
\emph{regularizes} that by the global parameters (another name for this is
\emph{shrinkage}).   
%The simplest approach to partial pooling is the `regression to the mean' idea
%of \cite{JORGENSEN2003}.  but calculated more systematically.

The partial pooling model will allow the regression model's slope and intercept
to vary. If our pooled regression equation predicts $y_i$, an observation at
point $i$, to be  $y_i = \alpha + \beta x_i + \epsilon_i$, than the partial
pooling regression equation is $y_i = \alpha_{j[i]} + \beta_{j[i]} x_{i} +
\epsilon_i$. The main difference with unpooled models is that we assign
hyperpriors to our parameters. 

The model component of the probabilistic program is 
\begin{verbatim}
model {
  mu_a ~ normal(0, 100); # hyperprior
  mu_b ~ normal(0, 100); # hyperprior
  a ~ normal(mu_a, sigma_a); # prior
  b ~ normal(mu_b, sigma_b); # prior
  # likelihood:
  y ~ normal(a[project] + b[project]*x, sigma); 
}
\end{verbatim}

with parameters $a,b$, priors for $y$, having posteriors that are normal, with
priors for the mean distributed as another normal model with mean 0 and sigma
100. This is an uninformative prior (allowing for a wide range of values, and
thus dominated by the data). Choosing priors effectively is a core skill in
probabilistic programming. 

\section{Results} 
An important part of Bayesian hierarchical modeling is model checking. This
involves inspection of the model inferences to ensure the fit is
reasonable. To do this we plot (Fig. \ref{fig:modelcheck}) the posterior distribution of our parameters and
the individual samples.

\begin{figure}[tb]
	\centering
	\includegraphics[width=.7\columnwidth]{eval-fit}
	\caption{Checking model convergence.}
	\label{fig:modelcheck}
\end{figure}

The left side shows our marginal posterior---for each parameter value on the
x-axis we get a probability on the y-axis that tells us how likely that
parameter value is. The right side shows the sampling chains. Our sampling
chains for the individual parameters (left side) seem well converged and
stationary (there are no large drifts or other odd patterns) \cite{Sorensen16}.

% \begin{figure}[b]
% 	\centering
% 	\includegraphics[width=.6\columnwidth]{slopes}
% 	\caption{Visualizing the varying slope/intercept model. Each line is a project-level regression line.}
% 	\label{fig:slopes}
% \end{figure}

% We can visualize the effect of a varying slope, varying intercept model in Fig. \ref{fig:slopes}. 
A sample of four projects in the SATT dataset is shown in Fig.
\ref{fig:partial-pool}. Conceptually, the regression is more accurate if the
right side data points (in blue dots) are closer to the regression line (x=1
being files which are Controllers). The dashed black line is the partial pooling
estimate; the dashed red line is the global/pooled estimate; and the solid blue
line the unpooled estimate. The global estimate seems way off. Depending on the
number of data values, the partial and unpooled estimates are fairly close. The
improvement in accuracy of the partial pooling approach increases as the number
of datapoints decreases (and the unpooled model gets worse). 

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{partial-pool.pdf}
  \caption{Comparing three models. Blue=unpooled, dotted black=partial pool, dashed red=pooled.
   N indicates number of controller files. X-axis reflects predictor of controller/not-controller.}
	\label{fig:partial-pool}
\end{figure}

%Validation is a bit trickier in hierarchical models. In K-Fold cross validation
%the temptation is to do this at the file level (i.e., hold back 10\% of the
%files for validation). The trouble is this may be unrepresentative of the
%higher levels of structure (the projects). Accordingly we hold back 1/5
\noindent\textbf{Validation}---We validate accuracy using a simple root mean
squared error (RMSE) approach.% with 5-fold cross-validation. 
%We pull out a random sample of individual points, stratified by project, 
We fit our three regression models, then calculate the average distance from the
predicted score to the actual file CBO values. We expect the partial-pooling to
have a lower RMSE value if it is more accurate than the others.
 %One of the nice benefits to the probabilistic programming approach we use is
 %the generation of standard errors as part of the sampling process. The error
 %for the pooled approach is simply the sigma portion of the model. 

Table \ref{tbl:rmse} reports the mean RMSE over all projects with Controllers
(n=115), and approximate Stan model training time with 2 chains of 1000
iterations (model compilation (to C++	) is a constant \emph{30s} or so in
addition). Partial pooling is \textbf{nearly twice as effective} as full
pooling, but takes 4 times as long (an OLS approach is nearly instantaneous and
produces the same RMSE as the full pooling model). No pooling is nearly as
effective as partial pooling (likely because the number of files that are
\emph{not} controllers is fairly large). However, error values for the unpooled
approach vary; in projects with small numbers of files, the RMSE for an unpooled
approach is much higher. This represents the benefits of shrinking towards the
global mean. For example, in the Netvogue project of Fig.
\ref{fig:partial-pool}, the unpooled regression (blue) is quite far from the
partial pooling line (black dashed). This is a difference in RMSE of
1.23 vs. 0.91.

\begin{table}
\begin{tabular}{c|c|c}
\toprule
Model 			& RMSE 	& Sampling Time \\ %& Variance 
\midrule
Full Pool 		& 1.099 & 26.9s \\ %&	1.37	
Unpooled 		& 0.630 & 5m11s \\ %&	1.35	
Partial Pool 	& 0.523 & 3m18s \\ %&	1.35	
\bottomrule
\end{tabular}
\caption{RMSE for 3 Regression Approaches}
\label{tbl:rmse}
\end{table}

\noindent\textbf{Justifying Thresholds}---The SATT finding \cite{Aniche2016} was
that a file's architectural role played a significant part in its eventual CBO
numbers. They used the procedure of Alves et al. \cite{alves2010deriving} to
identify thresholds. The Alves thresholds were set by ranking files (across all
projects) by lines of code (LOC), then finding the metric value of the file
closest to 70/80/90\% (moderate/high/very high risk). 
%The percentage is of the total lines of code. %Aniche et al. extend this,
% stratifying the files by architectural role, so they end up with 6 sets of 3
% thresholds, one set per architectural role. In our approach, we produce a
% regression model that produces a posterior probability distribution of CBO
% scores. We set our thresholds based on 70/80/90\% of the probability mass. To
% find if a given file violates the threshold, one looks up that project's
% predicted CBO for Controller or non-Controller files, finds the threshold
% predicted, and checks the file's actual CBO. In the models below we do not use
% LOC as a regression predictor, but that is a simple extension. 

We use our partial pooling regression model to set thresholds at 70/80/90\% of
the normal distribution of log CBO. E.g., for the project `V2V', the partial
pooling model tells us the project's mean LCBO value $y \sim
\mathcal{N}(a_\text{project} + b_\text{project}*x, \sigma)$. This normal
posterior distribution is used to find the point probability for our thresholds
$\tau$ (i.e. the point on the normal distribution holding $\tau$\% of the
probability mass). $\text{exp}(\tau)$ produces CBO score. For example, the `V2V'
project has expected Controller CBO thresholds of 32,49, and 88. Compare this to
another project, `tatami-team', with 21,32,58, respectively. Our thresholds vary
based on the project characteristics and the regularizing global dataset. Full
results for all projects are in our replication package (footnote p. 2).
% It is an open question to what extent there is a global CBO standard that
% ought to apply. %2gx_demo = 20,31,56

\section{Discussion and Related Work}
\label{related}
The chief threats to validity in Bayesian hierarchical modeling come from poor
understanding of the underlying probabilistic program, including what priors to
choose. Model checking is vital to mitigate this. There are subtleties in how
the sampler works that need careful checking, as shown by Wiecki's blog post
\cite{Wiecki17}. 

Other threats to validity include data analysis and implementation issues
(internal validity). One threat that is mitigated by this approach is external
validity, since a hierarchical model inherently accounts for inter-project
differences. Many of the common complaints about lack of generalizability (e.g.,
of Microsoft results to other companies) could be avoided with a hierarchical
approach.
% Indeed, additional levels are possible beyond files/projects (Spring, Java,
% etc).

This paper has only hinted at the uses for hierarchical modeling. Other useful
aspects include the ability to handle correlations between local and global
variables \cite{mcilreath16}, innately accounting for multiple-comparisons
problems \cite{Gelman:2012aa}, and adding group-level predictors (for example,
using number of files per project). Other aspects to explore include adding
additional predictors such as lines of code, different underlying prior
distributions, and improved analysis of model fit using Leave One Out (LOO) or
Widely Applicable Information Criterion (WAIC) \cite{vehtari15}. Although
consumer-ready, Bayesian inference remains unexplored compared to the highly
usable frequentist packages. Probabilistic programming is also an area that
merits more investigation as a form of software development in its own right.

\noindent\textbf{Related Work}---There are two main categories of \emph{related work.} 
%The background section earlier already introduced the concepts of Bayesian
%hierarchical modeling and probabilistic programs, so we avoid repeating that
%here. 
First, there is work that identifies the \textbf{differences between per-project
and pooled predictors} in analysing software metrics. Menzies et al.
\cite{menzies11ase} introduced the concept of `local' (clusters of data with
similar characteristics) and `global' (cross-project, pooled) lessons in defect
prediction. This idea has been followed up by many others (e.g.,
\cite{Kamei2016,Bettenburg2012,panichella14}). Posnett et al.
\cite{Posnett:2011} conducted a study comparing defect prediction at two levels:
aggregated (package) vs. unaggregated (file) and introduced the concept of
ecological inference. The innovation in our paper is to propose a partial
approach, regularizing the local predictions with the global information. Our
paper does not, however, make any claim to improve on defect prediction---we
leave that to future work. Several papers \cite{Bettenburg2012,Zhou15regress}
have leveraged linear (continuous response) and logistic regression (categorical
response); however, these are single-level models. 
%, in that they operate on predictors a pool of data (all files in a project,
%all projects in an ecosystem, etc). %Complex classifiers are also less
%explainable compared to hierarchical models, which clearly delineates the
%likelihood, priors and parameters.
Bettenburg et al. \cite{Bettenburg2012} blend the regression levels using a
different approach, Multivariate Adaptive Regression Splines (MARS). The MARS
approach appears to be fairly uncommon in the statistics literature, and uses
hinge functions to reduce the residual error. The Stan models we introduce are
simpler to specify, and the Bayesian approach to conditioning on the prior data
we find more intuitive.

% The Menzies et al. paper makes the case for considering both levels in
% analyzing metrics for defect prediction, and has been followed up with
% research into using \emph{transfer learning} for defect prediction
% \cite{ma2012transfer}. Transfer learning is similar to
% multi-level/hierarchical regression in fitting a model to a particular set of
% data. It then differs by learning key tuning parameters for the model when it
% is applied to new datasets. The intent is to avoid overfitting on sparse
% datasets by learning a generic model from more populous data. The two
% approaches share a similar goal; the main advantage to multi-level regression
% is that the models are conceptually simpler, and well supported by tooling.
% This is, however, a personal opinion. 

% A similar approach to ours---in that it uses linear regression---is found in
% \cite{Bettenburg2012}. They show that one challenge is identifying levels in
% software data, i.e., discovering sets of projects that can be said to be
% similar in some way. This \emph{clustering} problem is one our current paper
% does not tackle, relying as it does on pre-existing clusters (i.e., controller
% annotated Java files in Spring projects). Like previous work, they attack the
% problem with linear regression at two levels, `global' and `local'. However, 

Jørgenson et al. \cite{JORGENSEN2003} introduced a technique called `regression
to the mean' (RTM). It also attempts to regularize local predictions using
global parameters. 
%It does this by finding a global mean and adjusting local estimates to that mean. 
Hierarchical models are a more generalized and robust version of this, at the
expense of computation time.

There are few software engineering studies using hierarchical models. The two we
have identified are the 2012 work of Ehrlich and Cataldo
\cite{Ehrlich:2012}, who used multi-level (hierarchical) models to assess
communication in global software development. In 2017 Hassan et al.
\cite{Hassan2017} used a multi-level model to study app store reviews. This
approach is similar to this paper but uses a frequentist, maximum likelihood
approach, i.e., not Bayesian, using the `lme4' R package.
% Levels were individuals, teams, and iterations. %Their models are not
% specified in the text, but they fit a negative binomial regression model in
% what seems to be a frequentist approach (i.e., using maximum likelihood). 
We believe the Bayesian approach better accounts for prior information that is helpful when
 lacking observations.

 %NB In MLE, θ̂ is assumed to be a fixed quantity that is unknown but able to be
%  %inferred, not a random variable. Bayesian inference treats θ
%  as a random variable. Bayesian inference puts probability density functions in
%  and gets probability density functions out, rather than point summaries of the
%  model, as in MLE. That is, Bayesian inference looks at the full range of
%  parameter values and the probability of each. MLE posits that θ̂ is an adequate
%  summary of the data given the model
% With a uniform prior, MLE and MAP are the same.

Secondly, there is work that looks at \textbf{metric thresholds}. Since metrics
were proposed for software, researchers have sought to identify what thresholds
should trigger warnings. The paper we replicate from Aniche et al.
\cite{Aniche2016} has a good survey. We point out the work of Herraiz et al.
\cite{Herraiz2012}, Oliveira et al. \cite{oliveira14}, Foucault et al.
\cite{Foucault2014} and Alves et al. \cite{alves2010deriving} as more recent
examples. The metrics industry, e.g., SIG and CAST, has also long used databases
of company performance (a global pool) as benchmarks for evaluating new clients.

%TODO I have no good theory for the difference between a statistical classifier and linear regression

\section{Conclusion}
In this paper we introduced a hierarchical approach to setting metric
thresholds. Using a partial pooling approach, we are able to account for global
context, while retaining local variance. Our linear regression model defined a
project-specific mean LCBO score, which we used to set the benchmarks
accordingly. The hierarchical model is simple to describe, handles cross-project
comparisons, and is more accurate than a purely global or purely local approach.

\section{Acknowledgements}
We are grateful to the authors of \cite{Aniche2016}, who made their
well-packaged and documented data and code available for others.
 
\newpage
 \printbibliography 
\end{document}
